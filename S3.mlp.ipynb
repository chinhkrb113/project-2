{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e8a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9f9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccbea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._Y = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[\n",
    "                None,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        weights_1 = tf.get_variable(\n",
    "            name=\"weight_input_hidden\",\n",
    "            shape=(self._vocab_size, self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "        biases_1 = tf.get_variable(\n",
    "            name=\"biases_input_hidden\",\n",
    "            shape=(self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        weights_2 = tf.get_variable(\n",
    "            name=\"weights_hidden_output\",\n",
    "            shape=(self._hidden_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "        biases_2 = tf.get_variable(\n",
    "            name=\"biases_hidden_output\",\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "        hidden = tf.sigmoid(hidden)\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "        labels_one_hot = tf.one_hot(\n",
    "            indices=self._Y, depth=NUM_CLASSES, dtype=tf.float32\n",
    "        )\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=labels_one_hot, logits=logits\n",
    "        )\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        return predicted_labels, loss\n",
    "\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005069b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for line in d_lines:\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split(\"<fff>\")\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(\":\")[0]), float(token.split(\":\")[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "\n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = min(start + self._batch_size, len(self._data))\n",
    "        self._batch_id += 1\n",
    "\n",
    "        if end == len(self._data):\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2021)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de0ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(vocab_size):\n",
    "    train_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-train-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    test_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-test-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5854d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(\":\", \"-colon-\") + \"-epoch-{}.txt\".format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = \",\".join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = \"\\n\".join(\n",
    "            [\n",
    "                \",\".join([str(number) for number in value[row]])\n",
    "                for row in range(value.shape[0])\n",
    "            ]\n",
    "        )\n",
    "    with open(os.getcwd() + \"/saved-paras/\" + filename, \"w\") as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231c1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(\":\", \"-colon-\") + \"-epoch-{}.txt\".format(epoch)\n",
    "    with open(os.getcwd() + \"/saved-paras/\" + filename, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(\",\")]\n",
    "    else:\n",
    "        value = [\n",
    "            [float(number) for number in lines[row].split(\",\")]\n",
    "            for row in range(len(lines))\n",
    "        ]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "967f94ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 11.104410171508789\n",
      "step: 1, loss: 0.9813811779022217\n",
      "step: 2, loss: 0.0006089996313676238\n",
      "step: 3, loss: 2.036018940998474e-06\n",
      "step: 4, loss: 1.907348234908568e-08\n",
      "step: 5, loss: 0.0\n",
      "step: 6, loss: 0.0\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 14.78285026550293\n",
      "step: 10, loss: 35.11439514160156\n",
      "step: 11, loss: 31.772096633911133\n",
      "step: 12, loss: 26.245704650878906\n",
      "step: 13, loss: 20.384763717651367\n",
      "step: 14, loss: 12.772749900817871\n",
      "step: 15, loss: 6.130095958709717\n",
      "step: 16, loss: 0.9907452464103699\n",
      "step: 17, loss: 0.06624095141887665\n",
      "step: 18, loss: 0.00010842308984138072\n",
      "step: 19, loss: 1.053785354088177e-06\n",
      "step: 20, loss: 0.0\n",
      "step: 21, loss: 24.63418960571289\n",
      "step: 22, loss: 32.637168884277344\n",
      "step: 23, loss: 29.58039093017578\n",
      "step: 24, loss: 26.681072235107422\n",
      "step: 25, loss: 22.07748794555664\n",
      "step: 26, loss: 17.576826095581055\n",
      "step: 27, loss: 12.419458389282227\n",
      "step: 28, loss: 8.495311737060547\n",
      "step: 29, loss: 4.281156539916992\n",
      "step: 30, loss: 1.0771235227584839\n",
      "step: 31, loss: 0.04648755490779877\n",
      "step: 32, loss: 0.06662815809249878\n",
      "step: 33, loss: 17.661039352416992\n",
      "step: 34, loss: 19.809751510620117\n",
      "step: 35, loss: 18.3869686126709\n",
      "step: 36, loss: 16.34328842163086\n",
      "step: 37, loss: 14.193694114685059\n",
      "step: 38, loss: 11.481035232543945\n",
      "step: 39, loss: 8.820479393005371\n",
      "step: 40, loss: 6.483562469482422\n",
      "step: 41, loss: 4.291400909423828\n",
      "step: 42, loss: 2.3998427391052246\n",
      "step: 43, loss: 0.8817697167396545\n",
      "step: 44, loss: 1.789015531539917\n",
      "step: 45, loss: 15.761369705200195\n",
      "step: 46, loss: 14.412138938903809\n",
      "step: 47, loss: 14.547989845275879\n",
      "step: 48, loss: 12.690193176269531\n",
      "step: 49, loss: 11.249582290649414\n",
      "step: 50, loss: 9.52975082397461\n",
      "step: 51, loss: 7.860347747802734\n",
      "step: 52, loss: 6.2829155921936035\n",
      "step: 53, loss: 4.623212814331055\n",
      "step: 54, loss: 3.624711036682129\n",
      "step: 55, loss: 2.379946231842041\n",
      "step: 56, loss: 5.7982707023620605\n",
      "step: 57, loss: 7.969674587249756\n",
      "step: 58, loss: 7.420160293579102\n",
      "step: 59, loss: 6.651995658874512\n",
      "step: 60, loss: 5.669539928436279\n",
      "step: 61, loss: 4.632779121398926\n",
      "step: 62, loss: 3.7251784801483154\n",
      "step: 63, loss: 2.8136637210845947\n",
      "step: 64, loss: 2.0264458656311035\n",
      "step: 65, loss: 1.4851795434951782\n",
      "step: 66, loss: 0.9275365471839905\n",
      "step: 67, loss: 0.5142603516578674\n",
      "step: 68, loss: 6.940255165100098\n",
      "step: 69, loss: 10.02556324005127\n",
      "step: 70, loss: 9.5852689743042\n",
      "step: 71, loss: 9.182805061340332\n",
      "step: 72, loss: 8.593781471252441\n",
      "step: 73, loss: 7.833108901977539\n",
      "step: 74, loss: 7.125201225280762\n",
      "step: 75, loss: 6.336867809295654\n",
      "step: 76, loss: 5.343020439147949\n",
      "step: 77, loss: 4.7962446212768555\n",
      "step: 78, loss: 3.8779664039611816\n",
      "step: 79, loss: 3.3723254203796387\n",
      "step: 80, loss: 8.288186073303223\n",
      "step: 81, loss: 7.360991954803467\n",
      "step: 82, loss: 6.262778282165527\n",
      "step: 83, loss: 5.73477840423584\n",
      "step: 84, loss: 5.106266975402832\n",
      "step: 85, loss: 4.451647758483887\n",
      "step: 86, loss: 4.317210674285889\n",
      "step: 87, loss: 3.7200253009796143\n",
      "step: 88, loss: 3.5441503524780273\n",
      "step: 89, loss: 3.1838443279266357\n",
      "step: 90, loss: 2.9038591384887695\n",
      "step: 91, loss: 2.855570077896118\n",
      "step: 92, loss: 4.581860065460205\n",
      "step: 93, loss: 4.362789154052734\n",
      "step: 94, loss: 4.29393196105957\n",
      "step: 95, loss: 4.009975433349609\n",
      "step: 96, loss: 3.8524224758148193\n",
      "step: 97, loss: 3.6107146739959717\n",
      "step: 98, loss: 3.4679512977600098\n",
      "step: 99, loss: 3.244702100753784\n",
      "step: 100, loss: 3.014463424682617\n",
      "step: 101, loss: 2.8042643070220947\n",
      "step: 102, loss: 2.602804183959961\n",
      "step: 103, loss: 3.4255783557891846\n",
      "step: 104, loss: 9.667693138122559\n",
      "step: 105, loss: 9.371589660644531\n",
      "step: 106, loss: 9.159610748291016\n",
      "step: 107, loss: 8.986766815185547\n",
      "step: 108, loss: 8.76103687286377\n",
      "step: 109, loss: 8.623464584350586\n",
      "step: 110, loss: 8.202046394348145\n",
      "step: 111, loss: 8.02941608428955\n",
      "step: 112, loss: 7.658167839050293\n",
      "step: 113, loss: 7.41594123840332\n",
      "step: 114, loss: 7.117061614990234\n",
      "step: 115, loss: 7.296213150024414\n",
      "step: 116, loss: 9.09948444366455\n",
      "step: 117, loss: 8.868366241455078\n",
      "step: 118, loss: 8.647226333618164\n",
      "step: 119, loss: 8.425664901733398\n",
      "step: 120, loss: 8.142383575439453\n",
      "step: 121, loss: 7.921274185180664\n",
      "step: 122, loss: 7.577845573425293\n",
      "step: 123, loss: 7.280574798583984\n",
      "step: 124, loss: 6.969096660614014\n",
      "step: 125, loss: 6.360689163208008\n",
      "step: 126, loss: 5.9347004890441895\n",
      "step: 127, loss: 5.751931667327881\n",
      "step: 128, loss: 7.449451923370361\n",
      "step: 129, loss: 7.260941028594971\n",
      "step: 130, loss: 7.06060791015625\n",
      "step: 131, loss: 6.8036208152771\n",
      "step: 132, loss: 6.532404899597168\n",
      "step: 133, loss: 6.227334499359131\n",
      "step: 134, loss: 5.90782356262207\n",
      "step: 135, loss: 5.554502487182617\n",
      "step: 136, loss: 5.135533332824707\n",
      "step: 137, loss: 4.712714672088623\n",
      "step: 138, loss: 4.394723415374756\n",
      "step: 139, loss: 5.849916934967041\n",
      "step: 140, loss: 10.068353652954102\n",
      "step: 141, loss: 9.950401306152344\n",
      "step: 142, loss: 9.762154579162598\n",
      "step: 143, loss: 9.299692153930664\n",
      "step: 144, loss: 9.096571922302246\n",
      "step: 145, loss: 8.704146385192871\n",
      "step: 146, loss: 8.385233879089355\n",
      "step: 147, loss: 8.078741073608398\n",
      "step: 148, loss: 7.777426242828369\n",
      "step: 149, loss: 7.450247287750244\n",
      "step: 150, loss: 7.294277191162109\n",
      "step: 151, loss: 9.08122730255127\n",
      "step: 152, loss: 10.480144500732422\n",
      "step: 153, loss: 9.82612419128418\n",
      "step: 154, loss: 9.500131607055664\n",
      "step: 155, loss: 9.02538776397705\n",
      "step: 156, loss: 8.767556190490723\n",
      "step: 157, loss: 8.581268310546875\n",
      "step: 158, loss: 8.243197441101074\n",
      "step: 159, loss: 7.914386749267578\n",
      "step: 160, loss: 7.591395378112793\n",
      "step: 161, loss: 7.2960405349731445\n",
      "step: 162, loss: 6.973012924194336\n",
      "step: 163, loss: 8.467680931091309\n",
      "step: 164, loss: 9.585335731506348\n",
      "step: 165, loss: 9.351245880126953\n",
      "step: 166, loss: 9.118038177490234\n",
      "step: 167, loss: 8.911609649658203\n",
      "step: 168, loss: 8.597238540649414\n",
      "step: 169, loss: 8.280806541442871\n",
      "step: 170, loss: 7.961630821228027\n",
      "step: 171, loss: 7.559450626373291\n",
      "step: 172, loss: 7.258589267730713\n",
      "step: 173, loss: 6.859504222869873\n",
      "step: 174, loss: 6.126399517059326\n",
      "step: 175, loss: 5.511171817779541\n",
      "step: 176, loss: 4.8929972648620605\n",
      "step: 177, loss: 4.517350196838379\n",
      "step: 178, loss: 4.033370494842529\n",
      "step: 179, loss: 3.4328317642211914\n",
      "step: 180, loss: 2.884976863861084\n",
      "step: 181, loss: 2.2717573642730713\n",
      "step: 182, loss: 1.688159465789795\n",
      "step: 183, loss: 1.1551381349563599\n",
      "step: 184, loss: 0.7160581350326538\n",
      "step: 185, loss: 0.4286901354789734\n",
      "step: 186, loss: 0.2675267457962036\n",
      "step: 187, loss: 6.751935958862305\n",
      "step: 188, loss: 8.888954162597656\n",
      "step: 189, loss: 8.701715469360352\n",
      "step: 190, loss: 8.54298210144043\n",
      "step: 191, loss: 7.831505298614502\n",
      "step: 192, loss: 7.249422073364258\n",
      "step: 193, loss: 6.588356971740723\n",
      "step: 194, loss: 5.840157985687256\n",
      "step: 195, loss: 5.011842250823975\n",
      "step: 196, loss: 4.175629615783691\n",
      "step: 197, loss: 3.296595811843872\n",
      "step: 198, loss: 4.7221455574035645\n",
      "step: 199, loss: 4.688807010650635\n",
      "step: 200, loss: 4.201820373535156\n",
      "step: 201, loss: 3.721700429916382\n",
      "step: 202, loss: 3.2883782386779785\n",
      "step: 203, loss: 2.9188506603240967\n",
      "step: 204, loss: 2.435472249984741\n",
      "step: 205, loss: 1.9782885313034058\n",
      "step: 206, loss: 1.4801390171051025\n",
      "step: 207, loss: 1.0474052429199219\n",
      "step: 208, loss: 0.6867326498031616\n",
      "step: 209, loss: 7.435185432434082\n",
      "step: 210, loss: 12.858870506286621\n",
      "step: 211, loss: 12.701613426208496\n",
      "step: 212, loss: 12.478494644165039\n",
      "step: 213, loss: 12.038586616516113\n",
      "step: 214, loss: 11.33654499053955\n",
      "step: 215, loss: 10.099664688110352\n",
      "step: 216, loss: 9.347726821899414\n",
      "step: 217, loss: 8.038152694702148\n",
      "step: 218, loss: 9.52623462677002\n",
      "step: 219, loss: 15.281425476074219\n",
      "step: 220, loss: 13.099091529846191\n",
      "step: 221, loss: 12.193592071533203\n",
      "step: 222, loss: 10.697711944580078\n",
      "step: 223, loss: 10.141587257385254\n",
      "step: 224, loss: 9.650694847106934\n",
      "step: 225, loss: 9.306122779846191\n",
      "step: 226, loss: 3.3159050941467285\n",
      "step: 227, loss: 3.0706980228424072\n",
      "step: 228, loss: 2.7433252334594727\n",
      "step: 229, loss: 2.8925936222076416\n",
      "step: 230, loss: 2.883502244949341\n",
      "step: 231, loss: 2.9638521671295166\n",
      "step: 232, loss: 2.7212133407592773\n",
      "step: 233, loss: 2.82247257232666\n",
      "step: 234, loss: 3.558321475982666\n",
      "step: 235, loss: 2.606156587600708\n",
      "step: 236, loss: 2.9683022499084473\n",
      "step: 237, loss: 3.0703978538513184\n",
      "step: 238, loss: 3.067964792251587\n",
      "step: 239, loss: 2.859112024307251\n",
      "step: 240, loss: 2.6216955184936523\n",
      "step: 241, loss: 2.481170892715454\n",
      "step: 242, loss: 2.6207311153411865\n",
      "step: 243, loss: 2.694934606552124\n",
      "step: 244, loss: 2.5535457134246826\n",
      "step: 245, loss: 2.491227626800537\n",
      "step: 246, loss: 2.297499179840088\n",
      "step: 247, loss: 2.4329841136932373\n",
      "step: 248, loss: 2.1586356163024902\n",
      "step: 249, loss: 2.79045033454895\n",
      "step: 250, loss: 2.4315075874328613\n",
      "step: 251, loss: 2.134428024291992\n",
      "step: 252, loss: 2.226809501647949\n",
      "step: 253, loss: 2.038726568222046\n",
      "step: 254, loss: 2.343174934387207\n",
      "step: 255, loss: 2.0433359146118164\n",
      "step: 256, loss: 1.9554256200790405\n",
      "step: 257, loss: 2.183490037918091\n",
      "step: 258, loss: 2.486318349838257\n",
      "step: 259, loss: 2.0285706520080566\n",
      "step: 260, loss: 1.7579461336135864\n",
      "step: 261, loss: 1.9833160638809204\n",
      "step: 262, loss: 1.9177844524383545\n",
      "step: 263, loss: 1.8620901107788086\n",
      "step: 264, loss: 1.8552765846252441\n",
      "step: 265, loss: 1.820706844329834\n",
      "step: 266, loss: 1.4930163621902466\n",
      "step: 267, loss: 2.1526803970336914\n",
      "step: 268, loss: 1.8244746923446655\n",
      "step: 269, loss: 1.9756642580032349\n",
      "step: 270, loss: 2.0627198219299316\n",
      "step: 271, loss: 1.5928882360458374\n",
      "step: 272, loss: 1.4319965839385986\n",
      "step: 273, loss: 1.5567634105682373\n",
      "step: 274, loss: 1.536775827407837\n",
      "step: 275, loss: 1.8857853412628174\n",
      "step: 276, loss: 1.8771706819534302\n",
      "step: 277, loss: 1.3745524883270264\n",
      "step: 278, loss: 1.0902289152145386\n",
      "step: 279, loss: 1.4528619050979614\n",
      "step: 280, loss: 1.5016663074493408\n",
      "step: 281, loss: 1.3502384424209595\n",
      "step: 282, loss: 1.245017647743225\n",
      "step: 283, loss: 1.329105019569397\n",
      "step: 284, loss: 1.7621700763702393\n",
      "step: 285, loss: 0.8987538814544678\n",
      "step: 286, loss: 1.5216892957687378\n",
      "step: 287, loss: 1.3455945253372192\n",
      "step: 288, loss: 1.4678913354873657\n",
      "step: 289, loss: 1.2989327907562256\n",
      "step: 290, loss: 1.346643328666687\n",
      "step: 291, loss: 1.5865176916122437\n",
      "step: 292, loss: 1.4677807092666626\n",
      "step: 293, loss: 1.138195514678955\n",
      "step: 294, loss: 1.3946298360824585\n",
      "step: 295, loss: 1.2945767641067505\n",
      "step: 296, loss: 1.2630643844604492\n",
      "step: 297, loss: 1.0506819486618042\n",
      "step: 298, loss: 1.1318469047546387\n",
      "step: 299, loss: 1.5791393518447876\n",
      "step: 300, loss: 1.012937068939209\n",
      "step: 301, loss: 0.6551399827003479\n",
      "step: 302, loss: 1.0682377815246582\n",
      "step: 303, loss: 1.3319969177246094\n",
      "step: 304, loss: 1.2182308435440063\n",
      "step: 305, loss: 1.165274739265442\n",
      "step: 306, loss: 0.9858155250549316\n",
      "step: 307, loss: 0.8595612049102783\n",
      "step: 308, loss: 1.2636550664901733\n",
      "step: 309, loss: 0.8103819489479065\n",
      "step: 310, loss: 0.8890888690948486\n",
      "step: 311, loss: 1.3208421468734741\n",
      "step: 312, loss: 0.8275508880615234\n",
      "step: 313, loss: 0.6474900841712952\n",
      "step: 314, loss: 1.0810202360153198\n",
      "step: 315, loss: 1.1981273889541626\n",
      "step: 316, loss: 1.255008578300476\n",
      "step: 317, loss: 0.920360267162323\n",
      "step: 318, loss: 0.8994445204734802\n",
      "step: 319, loss: 1.0771515369415283\n",
      "step: 320, loss: 0.8922073841094971\n",
      "step: 321, loss: 0.7772216796875\n",
      "step: 322, loss: 0.9532890319824219\n",
      "step: 323, loss: 0.7973448038101196\n",
      "step: 324, loss: 0.7073734402656555\n",
      "step: 325, loss: 0.7939202785491943\n",
      "step: 326, loss: 0.6440103650093079\n",
      "step: 327, loss: 0.6003627181053162\n",
      "step: 328, loss: 0.9488903880119324\n",
      "step: 329, loss: 0.5716861486434937\n",
      "step: 330, loss: 1.1494431495666504\n",
      "step: 331, loss: 0.6961073875427246\n",
      "step: 332, loss: 0.7467730045318604\n",
      "step: 333, loss: 0.7087826728820801\n",
      "step: 334, loss: 0.8680190443992615\n",
      "step: 335, loss: 0.6391719579696655\n",
      "step: 336, loss: 0.8128147125244141\n",
      "step: 337, loss: 0.9938374161720276\n",
      "step: 338, loss: 0.5268042683601379\n",
      "step: 339, loss: 0.9562069177627563\n",
      "step: 340, loss: 0.7413842678070068\n",
      "step: 341, loss: 0.7361881136894226\n",
      "step: 342, loss: 0.7098052501678467\n",
      "step: 343, loss: 0.6871444582939148\n",
      "step: 344, loss: 0.6180101633071899\n",
      "step: 345, loss: 0.5932474732398987\n",
      "step: 346, loss: 0.5269649028778076\n",
      "step: 347, loss: 0.8712661862373352\n",
      "step: 348, loss: 1.0198636054992676\n",
      "step: 349, loss: 0.5640086531639099\n",
      "step: 350, loss: 0.9823655486106873\n",
      "step: 351, loss: 0.6086795330047607\n",
      "step: 352, loss: 0.49939998984336853\n",
      "step: 353, loss: 0.7245519161224365\n",
      "step: 354, loss: 0.9241719245910645\n",
      "step: 355, loss: 0.889794111251831\n",
      "step: 356, loss: 0.4551248848438263\n",
      "step: 357, loss: 0.7521223425865173\n",
      "step: 358, loss: 0.5007044076919556\n",
      "step: 359, loss: 0.6766093969345093\n",
      "step: 360, loss: 0.5844164490699768\n",
      "step: 361, loss: 0.5111148357391357\n",
      "step: 362, loss: 0.6669782996177673\n",
      "step: 363, loss: 0.8308413624763489\n",
      "step: 364, loss: 0.4791707992553711\n",
      "step: 365, loss: 0.9107868075370789\n",
      "step: 366, loss: 0.4511970281600952\n",
      "step: 367, loss: 0.4089112877845764\n",
      "step: 368, loss: 0.6710439920425415\n",
      "step: 369, loss: 0.4983884394168854\n",
      "step: 370, loss: 0.5303128361701965\n",
      "step: 371, loss: 0.49251940846443176\n",
      "step: 372, loss: 0.22464758157730103\n",
      "step: 373, loss: 0.36546340584754944\n",
      "step: 374, loss: 0.6154407858848572\n",
      "step: 375, loss: 0.5294888019561768\n",
      "step: 376, loss: 0.7769009470939636\n",
      "step: 377, loss: 0.6260603070259094\n",
      "step: 378, loss: 0.48938223719596863\n",
      "step: 379, loss: 0.6171098947525024\n",
      "step: 380, loss: 0.15452733635902405\n",
      "step: 381, loss: 0.9419383406639099\n",
      "step: 382, loss: 0.5244981646537781\n",
      "step: 383, loss: 0.8284374475479126\n",
      "step: 384, loss: 0.521969199180603\n",
      "step: 385, loss: 0.8756553530693054\n",
      "step: 386, loss: 0.45898351073265076\n",
      "step: 387, loss: 0.6034581661224365\n",
      "step: 388, loss: 0.4266625940799713\n",
      "step: 389, loss: 0.48265865445137024\n",
      "step: 390, loss: 0.5249063968658447\n",
      "step: 391, loss: 0.5575151443481445\n",
      "step: 392, loss: 0.39908501505851746\n",
      "step: 393, loss: 0.2828041911125183\n",
      "step: 394, loss: 0.7353340983390808\n",
      "step: 395, loss: 0.6220447421073914\n",
      "step: 396, loss: 0.7635786533355713\n",
      "step: 397, loss: 0.5780666470527649\n",
      "step: 398, loss: 0.42266976833343506\n",
      "step: 399, loss: 0.23936466872692108\n",
      "step: 400, loss: 0.441742867231369\n",
      "step: 401, loss: 0.7395939826965332\n",
      "step: 402, loss: 0.6404549479484558\n",
      "step: 403, loss: 0.28166449069976807\n",
      "step: 404, loss: 0.6529862284660339\n",
      "step: 405, loss: 0.3379383385181427\n",
      "step: 406, loss: 0.8721293807029724\n",
      "step: 407, loss: 0.5170445442199707\n",
      "step: 408, loss: 0.6962195038795471\n",
      "step: 409, loss: 0.91087806224823\n",
      "step: 410, loss: 0.3469870388507843\n",
      "step: 411, loss: 0.12005047500133514\n",
      "step: 412, loss: 0.3108135461807251\n",
      "step: 413, loss: 0.5388805270195007\n",
      "step: 414, loss: 0.3492206633090973\n",
      "step: 415, loss: 0.7595707178115845\n",
      "step: 416, loss: 0.4859681725502014\n",
      "step: 417, loss: 0.5397853255271912\n",
      "step: 418, loss: 0.3334696590900421\n",
      "step: 419, loss: 0.6233961582183838\n",
      "step: 420, loss: 0.44762539863586426\n",
      "step: 421, loss: 0.20627526938915253\n",
      "step: 422, loss: 0.17733292281627655\n",
      "step: 423, loss: 0.3887494206428528\n",
      "step: 424, loss: 0.4744265675544739\n",
      "step: 425, loss: 0.48711323738098145\n",
      "step: 426, loss: 0.3417990803718567\n",
      "step: 427, loss: 0.6587256789207458\n",
      "step: 428, loss: 0.4784682095050812\n",
      "step: 429, loss: 0.32347571849823\n",
      "step: 430, loss: 0.32226505875587463\n",
      "step: 431, loss: 0.41727665066719055\n",
      "step: 432, loss: 0.3883557915687561\n",
      "step: 433, loss: 0.6837413311004639\n",
      "step: 434, loss: 0.08492079377174377\n",
      "step: 435, loss: 0.2974061667919159\n",
      "step: 436, loss: 0.502373456954956\n",
      "step: 437, loss: 0.22893618047237396\n",
      "step: 438, loss: 0.6341650485992432\n",
      "step: 439, loss: 0.8467488288879395\n",
      "step: 440, loss: 0.6645688414573669\n",
      "step: 441, loss: 0.4451751410961151\n",
      "step: 442, loss: 0.37992027401924133\n",
      "step: 443, loss: 0.4837443232536316\n",
      "step: 444, loss: 0.48525047302246094\n",
      "step: 445, loss: 0.3353581130504608\n",
      "step: 446, loss: 0.2782398760318756\n",
      "step: 447, loss: 0.3835935592651367\n",
      "step: 448, loss: 0.3991428017616272\n",
      "step: 449, loss: 0.7190424203872681\n",
      "step: 450, loss: 0.22519731521606445\n",
      "step: 451, loss: 0.28863203525543213\n",
      "step: 452, loss: 0.2745397984981537\n",
      "step: 453, loss: 0.2250361442565918\n",
      "step: 454, loss: 0.39156705141067505\n",
      "step: 455, loss: 0.2242291122674942\n",
      "step: 456, loss: 0.17968155443668365\n",
      "step: 457, loss: 0.09737296402454376\n",
      "step: 458, loss: 0.10953868180513382\n",
      "step: 459, loss: 0.25644367933273315\n",
      "step: 460, loss: 0.11120391637086868\n",
      "step: 461, loss: 0.188939169049263\n",
      "step: 462, loss: 0.2197279930114746\n",
      "step: 463, loss: 0.05401438847184181\n",
      "step: 464, loss: 0.3985871970653534\n",
      "step: 465, loss: 0.35418379306793213\n",
      "step: 466, loss: 0.253159761428833\n",
      "step: 467, loss: 0.23518474400043488\n",
      "step: 468, loss: 0.14153192937374115\n",
      "step: 469, loss: 0.2771852910518646\n",
      "step: 470, loss: 0.2578039765357971\n",
      "step: 471, loss: 0.30716636776924133\n",
      "step: 472, loss: 0.10134806483983994\n",
      "step: 473, loss: 0.17307168245315552\n",
      "step: 474, loss: 0.04113910347223282\n",
      "step: 475, loss: 0.05435194447636604\n",
      "step: 476, loss: 0.17930975556373596\n",
      "step: 477, loss: 0.21551328897476196\n",
      "step: 478, loss: 0.11858361959457397\n",
      "step: 479, loss: 0.0635317862033844\n",
      "step: 480, loss: 0.1304147094488144\n",
      "step: 481, loss: 0.1051880270242691\n",
      "step: 482, loss: 0.19903986155986786\n",
      "step: 483, loss: 0.3376975655555725\n",
      "step: 484, loss: 0.0421147383749485\n",
      "step: 485, loss: 0.11467885971069336\n",
      "step: 486, loss: 0.23003768920898438\n",
      "step: 487, loss: 0.20664778351783752\n",
      "step: 488, loss: 0.21338121592998505\n",
      "step: 489, loss: 0.07234060019254684\n",
      "step: 490, loss: 0.1348203718662262\n",
      "step: 491, loss: 0.17246027290821075\n",
      "step: 492, loss: 0.2148011028766632\n",
      "step: 493, loss: 0.18050424754619598\n",
      "step: 494, loss: 0.28334686160087585\n",
      "step: 495, loss: 0.34399718046188354\n",
      "step: 496, loss: 0.2314458042383194\n",
      "step: 497, loss: 0.0483488030731678\n",
      "step: 498, loss: 0.2453196942806244\n",
      "step: 499, loss: 0.13525821268558502\n",
      "Epoch: 2\n",
      "Accuracy on test data: 0.7635422198619225\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 20\n",
    "\n",
    "# load features size\n",
    "with open(os.getcwd() + '/20news-bydate/20news-full-words-idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "\n",
    "# mlp initialization\n",
    "mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
    "\n",
    "# build graph model \n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "\n",
    "# optimizator\n",
    "train_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
    "\n",
    "# open a session\n",
    "with tf.Session() as sess:\n",
    "    # data reader initialization\n",
    "    train_data_reader, test_data_reader = load_dataset(vocab_size)\n",
    "    \n",
    "    # run session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    steps = 500\n",
    "    # training\n",
    "    for step in range(steps):\n",
    "        # load batch\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        \n",
    "        # feeding\n",
    "        labels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict={\n",
    "                mlp._X: train_data,\n",
    "                mlp._Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # results\n",
    "        print('step: {}, loss: {}'.format(step, loss_eval))\n",
    "        \n",
    "        # save params before train each epouch\n",
    "        if train_data_reader._batch_id == 0:\n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            for variable in trainable_variables:\n",
    "                save_parameters(\n",
    "                name = variable.name,\n",
    "                value = variable.eval(),\n",
    "                epoch = train_data_reader._num_epoch\n",
    "            )\n",
    "\n",
    "# load saved params\n",
    "with tf.Session() as sess:\n",
    "    # load final epoch params\n",
    "    epoch = train_data_reader._num_epoch\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name, epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "\n",
    "    # load one epoch to evaluate\n",
    "    num_true_preds = 0\n",
    "    while True:  \n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_labels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict = {\n",
    "                mlp._X: test_data,\n",
    "                mlp._Y: test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_labels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "    print('Epoch:', epoch)\n",
    "    print('Accuracy on test data:', num_true_preds/len(test_data_reader._data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
